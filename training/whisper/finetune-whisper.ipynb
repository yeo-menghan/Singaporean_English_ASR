{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\mh\\appdata\\local\\temp\\pip-req-build-xzny_xjp\n",
      "  Resolved https://github.com/huggingface/transformers to commit a40f1ac602fe900281722254c52ce3773f28eb0e\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.50.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\mh\\AppData\\Local\\Temp\\pip-req-build-xzny_xjp'\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from jiwer) (3.12.1)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (20240930)\n",
      "Requirement already satisfied: numba in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from openai-whisper) (0.61.0)\n",
      "Requirement already satisfied: numpy in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: torch in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from openai-whisper) (2.6.0+cu118)\n",
      "Requirement already satisfied: tqdm in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: more-itertools in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from openai-whisper) (10.6.0)\n",
      "Requirement already satisfied: tiktoken in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from openai-whisper) (0.9.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from numba->openai-whisper) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch->openai-whisper) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch->openai-whisper) (3.1.5)\n",
      "Requirement already satisfied: fsspec in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch->openai-whisper) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install librosa\n",
    "!pip install evaluate>=0.30\n",
    "!pip install jiwer\n",
    "!pip install -q bitsandbytes datasets accelerate\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main\n",
    "! pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: torchaudio in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from optuna) (24.2)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.38-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: tqdm in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "Downloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
      "Downloading SQLAlchemy-2.0.38-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 greenlet-3.1.1 optuna-4.2.1 sqlalchemy-2.0.38\n"
     ]
    }
   ],
   "source": [
    "! pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1763101b514f15ac2986f29f7162de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA device index\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "task = \"transcribe\"\n",
    "language = \"English\"\n",
    "model_name_or_path = \"openai/whisper-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is enabled and available!\n",
      "Using GPU: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is enabled and available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Falling back to CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  2 13:39:07 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060 ...  WDDM  |   00000000:26:00.0  On |                  N/A |\n",
      "| 38%   45C    P3             28W /  175W |     769MiB /   8192MiB |     37%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4480    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      7340    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A      8732    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A      9552    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10228    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A     11724    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     11824    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12068    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13828    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     14628    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     15964    C+G   ...b3d8bbwe\\Microsoft.Media.Player.exe      N/A      |\n",
      "|    0   N/A  N/A     16252    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     17336    C+G   ...B\\system_tray\\lghub_system_tray.exe      N/A      |\n",
      "|    0   N/A  N/A     17604    C+G   ...paper_engine\\bin\\webwallpaper32.exe      N/A      |\n",
      "|    0   N/A  N/A     18004    C+G   ...les (x86)\\Tencent\\QQLive\\QQLive.exe      N/A      |\n",
      "|    0   N/A  N/A     18376    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     19048    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     19528    C+G   ...on\\133.0.3065.92\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     20060    C+G   ...98.0_x64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A     21196    C+G   ...ncher\\NVIDIA Omniverse Launcher.exe      N/A      |\n",
      "|    0   N/A  N/A     21640    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22664    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     24152    C+G   ...-desktop\\RemoteDesktopCompanion.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Feature Extractor, Tokenizer and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: DatasetDict({\n",
      "        train: Dataset({\n",
      "            features: ['text', 'path'],\n",
      "            num_rows: 80\n",
      "        })\n",
      "    })\n",
      "    validation: DatasetDict({\n",
      "        train: Dataset({\n",
      "            features: ['text', 'path'],\n",
      "            num_rows: 10\n",
      "        })\n",
      "    })\n",
      "    test: DatasetDict({\n",
      "        train: Dataset({\n",
      "            features: ['text', 'path'],\n",
      "            num_rows: 10\n",
      "        })\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(\"..\", \"..\", \"data\", \"processed\"))\n",
    "\n",
    "# Load the dataset\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"csv\", data_files=os.path.join(data_dir, \"train_data.csv\")),\n",
    "    \"validation\": load_dataset(\"csv\", data_files=os.path.join(data_dir, \"validation_data.csv\")),\n",
    "    \"test\": load_dataset(\"csv\", data_files=os.path.join(data_dir, \"test_data.csv\"))\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Flatten the nested DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"][\"train\"],  # Extract the inner 'train' dataset\n",
    "    \"validation\": dataset[\"validation\"][\"train\"],  # Extract the inner 'train' dataset\n",
    "    \"test\": dataset[\"test\"][\"train\"]  # Extract the inner 'train' dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "Original path: ../processed\\test\\SP0873-CH00-SE01-RC755.flac\n",
      "New path: ../../data/processed/test/SP0873-CH00-SE01-RC755.flac\n",
      "New path: ../../data/processed/train/SP0449-CH00-SE00-RC107.flac\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(\"..\", \"..\", \"data\", \"processed\"))\n",
    "\n",
    "# Function to correct the file paths\n",
    "def correct_path(example):\n",
    "    # Convert Windows-style backslashes to forward slashes\n",
    "    current_path = example[\"path\"].replace(\"\\\\\", \"/\")\n",
    "    \n",
    "    # Remove '../processed' and replace with '../../data/processed'\n",
    "    if \"../processed\" in current_path:\n",
    "        new_path = current_path.replace(\"../processed\", \"../../data/processed\")\n",
    "    else:\n",
    "        new_path = current_path\n",
    "    \n",
    "    example[\"path\"] = new_path\n",
    "    return example\n",
    "\n",
    "# Apply the path correction to each split\n",
    "for split in dataset.keys():\n",
    "    dataset[split] = dataset[split].map(correct_path)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Verify the correction worked\n",
    "print(\"Original path:\", \"../processed\\\\test\\\\SP0873-CH00-SE01-RC755.flac\")\n",
    "print(\"New path:\", dataset[\"test\"][0][\"path\"])\n",
    "print(\"New path:\", dataset[\"train\"][0][\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "# Decode the audio column\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))  # Set the desired sampling rateD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '../../data/processed/train/SP0449-CH00-SE00-RC107.flac', 'array': array([-9.46044922e-04, -8.54492188e-04, -9.15527344e-05, ...,\n",
      "        5.49316406e-04,  6.10351562e-04,  6.10351562e-04]), 'sampling_rate': 16000}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0][\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "Dataset preparation complete!\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # Load and resample audio data\n",
    "    audio = batch[\"path\"]\n",
    "    \n",
    "    # Compute log-Mel input features\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    \n",
    "    # Encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "print(\"Preparing dataset...\")\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "print(\"Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a data collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Evaluation metric\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # Replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Handle different types of prediction outputs\n",
    "    if isinstance(pred_ids, list) and isinstance(pred_ids[0], list):\n",
    "        # Already in the right format\n",
    "        pass\n",
    "    elif hasattr(pred_ids, \"shape\") and len(pred_ids.shape) > 1:\n",
    "        # Convert numpy arrays to lists if needed\n",
    "        pred_ids = pred_ids.tolist()\n",
    "    \n",
    "    # We don't want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained model and apply post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Set decoding parameters\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mh\\AppData\\Local\\Temp\\ipykernel_12480\\3450653568.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "[I 2025-03-02 14:28:06,077] A new study created in memory with name: no-name-6b2040a9-afb2-4805-a305-c2fab41e445e\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 01:30, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.483379</td>\n",
       "      <td>17.218543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.457800</td>\n",
       "      <td>0.350180</td>\n",
       "      <td>17.880795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.291459</td>\n",
       "      <td>16.556291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.278229</td>\n",
       "      <td>15.894040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[I 2025-03-02 14:29:40,005] Trial 0 finished with value: 15.894039735099339 and parameters: {'learning_rate': 0.00018202941353713387, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 1, 'num_train_epochs': 4, 'warmup_ratio': 0.07875250713094617, 'weight_decay': 0.059292149524331775}. Best is trial 0 with value: 15.894039735099339.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.557119</td>\n",
       "      <td>74.834437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.488122</td>\n",
       "      <td>66.887417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[I 2025-03-02 14:30:33,703] Trial 1 finished with value: 66.88741721854305 and parameters: {'learning_rate': 4.506003833453285e-05, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 2, 'num_train_epochs': 2, 'warmup_ratio': 0.1871436737010505, 'weight_decay': 0.001992600458771887}. Best is trial 0 with value: 15.894039735099339.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.425882</td>\n",
       "      <td>60.927152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.564951</td>\n",
       "      <td>19.205298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.473900</td>\n",
       "      <td>0.484814</td>\n",
       "      <td>17.880795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.473900</td>\n",
       "      <td>0.435242</td>\n",
       "      <td>17.880795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[I 2025-03-02 14:32:02,200] Trial 2 finished with value: 17.880794701986755 and parameters: {'learning_rate': 0.00015822925797722426, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 1, 'num_train_epochs': 4, 'warmup_ratio': 0.20476476691299872, 'weight_decay': 0.03385213435331902}. Best is trial 0 with value: 15.894039735099339.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.480598</td>\n",
       "      <td>17.880795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297026</td>\n",
       "      <td>17.218543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.979100</td>\n",
       "      <td>0.257361</td>\n",
       "      <td>13.907285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.979100</td>\n",
       "      <td>0.248670</td>\n",
       "      <td>13.907285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.240209</td>\n",
       "      <td>15.231788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[I 2025-03-02 14:33:47,679] Trial 3 finished with value: 15.2317880794702 and parameters: {'learning_rate': 0.0005288644842478324, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 1, 'num_train_epochs': 5, 'warmup_ratio': 0.1378327739901833, 'weight_decay': 0.009531334178799755}. Best is trial 3 with value: 15.2317880794702.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 01:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.535975</td>\n",
       "      <td>70.860927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.348417</td>\n",
       "      <td>58.278146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.878400</td>\n",
       "      <td>1.193868</td>\n",
       "      <td>54.966887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.878400</td>\n",
       "      <td>1.104364</td>\n",
       "      <td>31.125828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[I 2025-03-02 14:35:31,737] Trial 4 finished with value: 31.125827814569533 and parameters: {'learning_rate': 6.341633181972459e-05, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 2, 'num_train_epochs': 4, 'warmup_ratio': 0.17205483368925922, 'weight_decay': 0.08502806904341385}. Best is trial 3 with value: 15.2317880794702.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/80 00:20 < 01:08, 0.88 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.617794</td>\n",
       "      <td>49.668874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 14:35:55,607] Trial 5 pruned. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:37, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.189519</td>\n",
       "      <td>17.218543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.538009</td>\n",
       "      <td>19.205298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[I 2025-03-02 14:36:42,855] Trial 6 finished with value: 19.205298013245034 and parameters: {'learning_rate': 0.0008339409498623009, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 2, 'num_train_epochs': 3, 'warmup_ratio': 0.0653402960047252, 'weight_decay': 0.06854645653601914}. Best is trial 3 with value: 15.2317880794702.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/25 00:45 < 01:25, 0.18 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.646282</td>\n",
       "      <td>80.794702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.607569</td>\n",
       "      <td>90.728477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "[I 2025-03-02 14:37:34,373] Trial 7 pruned. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/30 00:22 < 00:55, 0.36 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.545834</td>\n",
       "      <td>74.834437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 14:38:00,649] Trial 8 pruned. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/4 00:15 < 00:15, 0.07 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.645508</td>\n",
       "      <td>78.807947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 14:38:25,622] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.0005288644842478324, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 1, 'num_train_epochs': 5, 'warmup_ratio': 0.1378327739901833, 'weight_decay': 0.009531334178799755}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.483654</td>\n",
       "      <td>17.880795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.295168</td>\n",
       "      <td>17.218543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>0.255346</td>\n",
       "      <td>13.907285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>0.248491</td>\n",
       "      <td>14.569536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.240733</td>\n",
       "      <td>15.231788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.5772707080841064, metrics={'train_runtime': 111.2445, 'train_samples_per_second': 3.596, 'train_steps_per_second': 0.449, 'total_flos': 4.79222562816e+16, 'train_loss': 0.5772707080841064, 'epoch': 5.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers.trainer_utils import HPSearchBackend\n",
    "import optuna\n",
    "\n",
    "def model_init():\n",
    "    # Initialize a fresh model for each trial\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        model_name_or_path, \n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Set decoding parameters\n",
    "    model.config.use_cache = False\n",
    "    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2, 4]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.3),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.1),\n",
    "    }\n",
    "\n",
    "# Define the baseline training arguments (hyperparameters will be updated by search)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../../output/models/whisper/hp_search\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    "    logging_steps=25,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_wer\",  # WER (lower is better)\n",
    "    greater_is_better=False,  # For WER, lower is better\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer with model_init function\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model_init=model_init,  # Important: use model_init instead of model\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "\n",
    "# Run hyperparameter search (n_trials specifies how many combinations to try)\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"minimize\",  # Minimize WER\n",
    "    backend=\"optuna\",\n",
    "    hp_space=hp_space,\n",
    "    n_trials=10,  # Adjust based on your computational resources\n",
    "    compute_objective=lambda metrics: metrics[\"eval_wer\"],\n",
    ")\n",
    "\n",
    "print(f\"Best hyperparameters: {best_trial.hyperparameters}\")\n",
    "\n",
    "# Train with the best hyperparameters\n",
    "for param, value in best_trial.hyperparameters.items():\n",
    "    setattr(trainer.args, param, value)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 73,773,568 || trainable%: 1.5990\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # For Whisper architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# Update model configuration before training\n",
    "model.config.use_cache = False\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# Define a more robust training arguments setup\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../../output/models/whisper/\",\n",
    "    per_device_train_batch_size=8,  # Reduced from 16 to potentially avoid memory issues\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=4,  \n",
    "    predict_with_generate=True,  # Use generation during evaluation\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,  # Simple greedy decoding to avoid memory issues\n",
    "    logging_steps=25,\n",
    "    remove_unused_columns=False,  # Required for PEFT models\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "# Callback to save PEFT adapter weights\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "        \n",
    "        # Save the adapter model\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "        \n",
    "        # Remove the PyTorch model file which is not needed for PEFT\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mh\\AppData\\Local\\Temp\\ipykernel_12480\\3443089753.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,  # Use the full processor instead of just the feature extractor\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.658005</td>\n",
       "      <td>70.860927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.364233</td>\n",
       "      <td>70.198675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.555613</td>\n",
       "      <td>18.543046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Disable caching during training to avoid memory issues\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save_pretrained(\"../../output/models/whisper/output_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 21.93%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "eval_results = trainer.evaluate(dataset[\"test\"])\n",
    "print(f\"Test WER: {eval_results['eval_wer']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally run inference examples\n",
    "def transcribe_audio(audio_path):\n",
    "    # Load audio\n",
    "    from datasets import load_dataset\n",
    "    audio = load_dataset(\"audiofolder\", data_dir=os.path.dirname(audio_path), \n",
    "                          split=\"train\", drop_labels=True)\n",
    "    audio = audio[0][\"audio\"]\n",
    "    \n",
    "    # Process audio\n",
    "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "    \n",
    "    # Generate\n",
    "    model.config.use_cache = True  # Enable cache for inference\n",
    "    generated_ids = model.generate(input_features=input_features.to(\"cuda\"))\n",
    "    \n",
    "    # Decode\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio = \"./data/processed/samples/test_sample.wav\"\n",
    "print(f\"Sample transcription: {transcribe_audio(sample_audio)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
