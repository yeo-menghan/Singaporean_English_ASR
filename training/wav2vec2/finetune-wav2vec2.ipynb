{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  2 17:38:35 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2060 ...  WDDM  |   00000000:26:00.0  On |                  N/A |\n",
      "| 55%   57C    P2             41W /  175W |     670MiB /   8192MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1052      C   ...rograms\\Python\\Python310\\python.exe      N/A      |\n",
      "|    0   N/A  N/A      5816    C+G   ...paper_engine\\bin\\webwallpaper32.exe      N/A      |\n",
      "|    0   N/A  N/A      7340    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A      8732    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "|    0   N/A  N/A      9552    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10228    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A     11724    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     12068    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13828    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     14628    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     15964    C+G   ...b3d8bbwe\\Microsoft.Media.Player.exe      N/A      |\n",
      "|    0   N/A  N/A     16252    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     17336    C+G   ...B\\system_tray\\lghub_system_tray.exe      N/A      |\n",
      "|    0   N/A  N/A     18004    C+G   ...les (x86)\\Tencent\\QQLive\\QQLive.exe      N/A      |\n",
      "|    0   N/A  N/A     18376    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     19048    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     19528    C+G   ...on\\133.0.3065.92\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     19712    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     20060    C+G   ...98.0_x64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A     21196    C+G   ...ncher\\NVIDIA Omniverse Launcher.exe      N/A      |\n",
      "|    0   N/A  N/A     21640    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22664    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     24152    C+G   ...-desktop\\RemoteDesktopCompanion.exe      N/A      |\n",
      "|    0   N/A  N/A     26288    C+G   ...aming\\Telegram Desktop\\Telegram.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==1.18.3\n",
      "  Downloading datasets-1.18.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (2.1.3)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (19.0.1)\n",
      "Requirement already satisfied: dill in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (4.67.1)\n",
      "Requirement already satisfied: xxhash in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->datasets==1.18.3) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (0.29.1)\n",
      "Requirement already satisfied: packaging in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets==1.18.3) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets==1.18.3) (1.18.3)\n",
      "Requirement already satisfied: filelock in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (3.17.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.18.3) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==1.18.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==1.18.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==1.18.3) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==1.18.3) (2025.1.31)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets==1.18.3) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas->datasets==1.18.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas->datasets==1.18.3) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas->datasets==1.18.3) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==1.18.3) (1.17.0)\n",
      "Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.3.2\n",
      "    Uninstalling datasets-3.3.2:\n",
      "      Successfully uninstalled datasets-3.3.2\n",
      "Successfully installed datasets-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "evaluate 0.4.3 requires datasets>=2.0.0, but you have datasets 1.18.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.17.0\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: filelock in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (2.32.3)\n",
      "Requirement already satisfied: sacremoses in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (0.1.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from transformers==4.17.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.17.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.17.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.17.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->transformers==4.17.0) (2025.1.31)\n",
      "Requirement already satisfied: click in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from sacremoses->transformers==4.17.0) (8.1.8)\n",
      "Requirement already satisfied: joblib in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from sacremoses->transformers==4.17.0) (1.4.2)\n",
      "Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 20.7 MB/s eta 0:00:00\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.50.0.dev0\n",
      "    Uninstalling transformers-4.50.0.dev0:\n",
      "      Successfully uninstalled transformers-4.50.0.dev0\n",
      "Successfully installed transformers-4.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from jiwer) (3.12.1)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==1.18.3\n",
    "!pip install transformers==4.17.0\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: git-lfs in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (1.6)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from soundfile) (2.1.3)\n",
      "Requirement already satisfied: pycparser in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (e:\\singaporean_english_asr\\.venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6cee6d87604317b394a794c929e3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'path'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(\"..\", \"..\", \"data\", \"processed\"))\n",
    "\n",
    "# Load the dataset using pandas first, then convert to datasets\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train_data.csv\"))\n",
    "validation_df = pd.read_csv(os.path.join(data_dir, \"validation_data.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"))\n",
    "\n",
    "# Convert pandas DataFrames to datasets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(validation_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function load_audio at 0x00000230B340C1F0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecd9690aaf4497dbe27fb651690493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6173c95d754325868c1f68933b60ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375b271c889d42e3ae5c8f1d8b57e51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_audio(batch):\n",
    "    # Import inside the function to ensure it's available in worker processes\n",
    "    import soundfile as sf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    # Make sure the path is absolute\n",
    "    audio_path = batch[\"path\"]\n",
    "    \n",
    "    # Fix path if needed (check if the path exists)\n",
    "    if not os.path.isfile(audio_path):\n",
    "        # Try to fix relative path issues\n",
    "        base_dir = os.path.abspath(os.path.join(\"..\", \"..\", \"data\", \"processed\"))\n",
    "        audio_path = os.path.join(base_dir, audio_path.replace(\"../processed\\\\\", \"\"))\n",
    "    \n",
    "    # Load audio from the file path\n",
    "    try:\n",
    "        audio_array, sampling_rate = sf.read(audio_path)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if len(audio_array.shape) > 1:\n",
    "            audio_array = audio_array.mean(axis=1)\n",
    "        \n",
    "        batch[\"audio\"] = {\"array\": audio_array, \"sampling_rate\": sampling_rate}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        # Return a default audio array to avoid failing\n",
    "        batch[\"audio\"] = {\"array\": np.zeros(16000), \"sampling_rate\": 16000}\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Apply the audio loading to all splits, with fewer processes\n",
    "dataset = dataset.map(load_audio, num_proc=1)  # Start with 1 process to debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e1aa57a32a4a85bdc786204ae7b086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da0c80b87f64261a103538bdd822c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42980b76ac51415fa40d274377c797fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\\\,\\\\?\\\\.\\\\!\\\\-\\\\;\\\\:\\\\\\\"]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153fa9e7c50748b4af5f3744ea3ae5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dede6e2c4244a84a219b3d1aa3fa3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cac21a91197498cb481d8351231436d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32\n"
     ]
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, \n",
    "                    remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"validation\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "print(f\"Vocabulary size: {len(vocab_dict)}\")\n",
    "\n",
    "# Add special tokens\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token id: 33\n",
      "Vocabulary size: 34\n",
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:13<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 21.98it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31dd542ac774f0fba5d460ccd05b92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Create the processor\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "print(f\"Pad token id: {processor.tokenizer.pad_token_id}\")\n",
    "print(f\"Vocabulary size: {len(processor.tokenizer.get_vocab())}\")\n",
    "\n",
    "# Create a new processed dataset\n",
    "processed_datasets = DatasetDict()\n",
    "\n",
    "# Process each split sequentially\n",
    "for split in dataset:\n",
    "    print(f\"Processing {split} split...\")\n",
    "    processed_examples = []\n",
    "    \n",
    "    for i, example in enumerate(tqdm(dataset[split])):\n",
    "        try:\n",
    "            audio = example[\"audio\"]\n",
    "            \n",
    "            # Process audio\n",
    "            input_values = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "            input_length = len(input_values)\n",
    "            \n",
    "            # Process text\n",
    "            with processor.as_target_processor():\n",
    "                labels = processor(example[\"text\"]).input_ids\n",
    "            \n",
    "            # Create a new example with the processed data\n",
    "            processed_example = {\n",
    "                \"input_values\": input_values,\n",
    "                \"input_length\": input_length,\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "            processed_examples.append(processed_example)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i} in {split}: {e}\")\n",
    "    \n",
    "    # Convert list of dictionaries to dataset\n",
    "    processed_datasets[split] = Dataset.from_pandas(pd.DataFrame(processed_examples))\n",
    "\n",
    "# Replace the original dataset\n",
    "dataset = processed_datasets\n",
    "\n",
    "# Filter out examples that are too long \n",
    "max_input_length_in_sec = 10.0  # Adjust based on your data\n",
    "if \"input_length\" in dataset[\"train\"].column_names:\n",
    "    dataset[\"train\"] = dataset[\"train\"].filter(\n",
    "        lambda x: x < max_input_length_in_sec * processor.feature_extractor.sampling_rate, \n",
    "        input_columns=[\"input_length\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data_collator before using it\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        \n",
    "        # Make sure labels only contain valid indices\n",
    "        label_features = []\n",
    "        for feature in features:\n",
    "            # Ensure all label values are valid (below vocab size)\n",
    "            labels = [l for l in feature[\"labels\"] if l < len(processor.tokenizer)]\n",
    "            if not labels:  # If empty after filtering, add pad token\n",
    "                labels = [processor.tokenizer.pad_token_id]\n",
    "            label_features.append({\"input_ids\": labels})\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly (except for blank/pad token)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1) & (labels_batch[\"input_ids\"] != processor.tokenizer.pad_token_id), \n",
    "            -100\n",
    "        )\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics function\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/config.json from cache at C:\\Users\\mh/.cache\\huggingface\\transformers\\c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\configuration_utils.py:356: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config Wav2Vec2Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 33,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 34,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/wav2vec2-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\mh/.cache\\huggingface\\transformers\\ef45231897ce572a660ebc5a63d3702f1a6041c4c5fb78cbec330708531939b3.fcae05302a685f7904c551c8ea571e8bc2a2c4a1777ea81ad66e47f7883a650a\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.bias', 'quantizer.codevectors', 'project_hid.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config pad_token_id: 33\n",
      "Model config vocab_size: 34\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.abspath(os.path.join(\"..\", \"..\", \"output\", \"models\", \"wav2vec2\"))\n",
    "best_model_dir = os.path.abspath(\"../../output/models/wav2vec2/best_model\")\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(best_model_dir, exist_ok=True)\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\",\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer)\n",
    ")\n",
    "\n",
    "# Double-check that the model's config matches your tokenizer\n",
    "print(f\"Model config pad_token_id: {model.config.pad_token_id}\")\n",
    "print(f\"Model config vocab_size: {model.config.vocab_size}\")\n",
    "\n",
    "# If needed, manually set the CTC blank index to match pad_token_id\n",
    "if hasattr(model, 'config'):\n",
    "    model.config.blank_index = processor.tokenizer.pad_token_id\n",
    "\n",
    "model.freeze_feature_encoder()  # Freeze the feature encoder for more efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from transformers import TrainerCallback \n",
    "\n",
    "\n",
    "# Create a custom callback to track metrics\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.metrics = {\"epoch\": [], \"train_loss\": [], \"eval_loss\": [], \"eval_wer\": []}\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        # Save the evaluation metrics\n",
    "        if \"eval_loss\" in metrics:\n",
    "            self.metrics[\"eval_loss\"].append(metrics[\"eval_loss\"])\n",
    "            self.metrics[\"eval_wer\"].append(metrics[\"eval_wer\"])\n",
    "            self.metrics[\"epoch\"].append(state.epoch)\n",
    "            \n",
    "            # If we have train loss from the last log, record it too\n",
    "            if hasattr(state, \"log_history\") and len(state.log_history) > 0:\n",
    "                for entry in reversed(state.log_history):\n",
    "                    if \"loss\" in entry:\n",
    "                        self.metrics[\"train_loss\"].append(entry[\"loss\"])\n",
    "                        break\n",
    "                else:\n",
    "                    # If no train loss found, use NaN\n",
    "                    self.metrics[\"train_loss\"].append(float(\"nan\"))\n",
    "            \n",
    "            # Display current metrics as a table\n",
    "            metrics_df = pd.DataFrame(self.metrics)\n",
    "            display(metrics_df)\n",
    "            \n",
    "            # Save the metrics to CSV\n",
    "            metrics_df.to_csv(os.path.join(output_dir, \"training_metrics.csv\"), index=False)\n",
    "\n",
    "# Create the metrics callback\n",
    "metrics_callback = MetricsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=15,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.005,\n",
    "    warmup_steps=1000,\n",
    "    save_total_limit=3,  # Keep only the 3 best checkpoints\n",
    "    push_to_hub=False,\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"wer\",  # Use WER for determining the best model\n",
    "    greater_is_better=False,      # Lower WER is better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:474: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[metrics_callback],  # Add our custom callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 79\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee108f5e53e647d2bc9fdad280b4d2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a87c25bbd4e47149a6c7bba8d3199ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.716703414916992, 'eval_wer': 1.0, 'eval_runtime': 0.8783, 'eval_samples_per_second': 11.386, 'eval_steps_per_second': 2.277, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-10\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-10\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-10\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-10\\preprocessor_config.json\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7818597fe1124d3b8d407bacca5b7fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.425539016723633, 'eval_wer': 1.0, 'eval_runtime': 0.8547, 'eval_samples_per_second': 11.7, 'eval_steps_per_second': 2.34, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-20\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-20\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-20\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-20\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-30] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c2e1cdcd1a4c299e9499148c3d1ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.6627197265625, 'eval_wer': 1.0, 'eval_runtime': 0.8327, 'eval_samples_per_second': 12.009, 'eval_steps_per_second': 2.402, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-30\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-30\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-30\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-30\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-150] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acd4a2ce7984ffa8bf9241f6d9ae866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.775192260742188, 'eval_wer': 1.0, 'eval_runtime': 0.9753, 'eval_samples_per_second': 10.253, 'eval_steps_per_second': 2.051, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0\n",
       "3    4.0         NaN   9.775192       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-40\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-40\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-40\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-40\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-20] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed555fdc919f472bb2f781ac6edb9170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.321245193481445, 'eval_wer': 1.0, 'eval_runtime': 0.8217, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 2.434, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0\n",
       "3    4.0         NaN   9.775192       1.0\n",
       "4    5.0         NaN   8.321245       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-50\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-50\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-50\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-50\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-30] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ba0e4ad0c94dfab947bb4187941916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.988673210144043, 'eval_wer': 1.0, 'eval_runtime': 0.8562, 'eval_samples_per_second': 11.679, 'eval_steps_per_second': 2.336, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0\n",
       "3    4.0         NaN   9.775192       1.0\n",
       "4    5.0         NaN   8.321245       1.0\n",
       "5    6.0         NaN   6.988673       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-60\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-60\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-60\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-60\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-40] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acac11c69f494d57ad2305667218ef8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.790219306945801, 'eval_wer': 1.0, 'eval_runtime': 0.7942, 'eval_samples_per_second': 12.592, 'eval_steps_per_second': 2.518, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0\n",
       "3    4.0         NaN   9.775192       1.0\n",
       "4    5.0         NaN   8.321245       1.0\n",
       "5    6.0         NaN   6.988673       1.0\n",
       "6    7.0         NaN   4.790219       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-70\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-70\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-70\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-70\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-50] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e08c808f1647cb9cf47f2d48570a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.974931001663208, 'eval_wer': 1.0, 'eval_runtime': 0.7987, 'eval_samples_per_second': 12.521, 'eval_steps_per_second': 2.504, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0\n",
       "3    4.0         NaN   9.775192       1.0\n",
       "4    5.0         NaN   8.321245       1.0\n",
       "5    6.0         NaN   6.988673       1.0\n",
       "6    7.0         NaN   4.790219       1.0\n",
       "7    8.0         NaN   2.974931       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-80\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-80\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-80\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-80\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-60] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb8a2f4e6bd42b28dda6202856cf487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.920100450515747, 'eval_wer': 1.0, 'eval_runtime': 0.7687, 'eval_samples_per_second': 13.01, 'eval_steps_per_second': 2.602, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0\n",
       "3    4.0         NaN   9.775192       1.0\n",
       "4    5.0         NaN   8.321245       1.0\n",
       "5    6.0         NaN   6.988673       1.0\n",
       "6    7.0         NaN   4.790219       1.0\n",
       "7    8.0         NaN   2.974931       1.0\n",
       "8    9.0         NaN   1.920100       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-90\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-90\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-90\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-90\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-70] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7177, 'learning_rate': 9.800000000000001e-06, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f499c55685e4a968a53c895c667324a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6077340841293335, 'eval_wer': 1.0, 'eval_runtime': 0.7697, 'eval_samples_per_second': 12.993, 'eval_steps_per_second': 2.599, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.607734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  eval_loss  eval_wer\n",
       "0    1.0         NaN  11.716703       1.0\n",
       "1    2.0         NaN  11.425539       1.0\n",
       "2    3.0         NaN  10.662720       1.0\n",
       "3    4.0         NaN   9.775192       1.0\n",
       "4    5.0         NaN   8.321245       1.0\n",
       "5    6.0         NaN   6.988673       1.0\n",
       "6    7.0         NaN   4.790219       1.0\n",
       "7    8.0         NaN   2.974931       1.0\n",
       "8    9.0         NaN   1.920100       1.0\n",
       "9   10.0      6.7177   1.607734       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-100\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-100\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-100\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-100\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-80] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c4703de43b49608c48bb102bb62f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4986822605133057, 'eval_wer': 1.0, 'eval_runtime': 0.8848, 'eval_samples_per_second': 11.303, 'eval_steps_per_second': 2.261, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.607734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.498682</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  eval_loss  eval_wer\n",
       "0     1.0         NaN  11.716703       1.0\n",
       "1     2.0         NaN  11.425539       1.0\n",
       "2     3.0         NaN  10.662720       1.0\n",
       "3     4.0         NaN   9.775192       1.0\n",
       "4     5.0         NaN   8.321245       1.0\n",
       "5     6.0         NaN   6.988673       1.0\n",
       "6     7.0         NaN   4.790219       1.0\n",
       "7     8.0         NaN   2.974931       1.0\n",
       "8     9.0         NaN   1.920100       1.0\n",
       "9    10.0      6.7177   1.607734       1.0\n",
       "10   11.0      6.7177   1.498682       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-110\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-110\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-110\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-110\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-90] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8acc59a6f3f4082965a4fc976a861de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.438489556312561, 'eval_wer': 1.0, 'eval_runtime': 0.7837, 'eval_samples_per_second': 12.76, 'eval_steps_per_second': 2.552, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.607734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.498682</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.438490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  eval_loss  eval_wer\n",
       "0     1.0         NaN  11.716703       1.0\n",
       "1     2.0         NaN  11.425539       1.0\n",
       "2     3.0         NaN  10.662720       1.0\n",
       "3     4.0         NaN   9.775192       1.0\n",
       "4     5.0         NaN   8.321245       1.0\n",
       "5     6.0         NaN   6.988673       1.0\n",
       "6     7.0         NaN   4.790219       1.0\n",
       "7     8.0         NaN   2.974931       1.0\n",
       "8     9.0         NaN   1.920100       1.0\n",
       "9    10.0      6.7177   1.607734       1.0\n",
       "10   11.0      6.7177   1.498682       1.0\n",
       "11   12.0      6.7177   1.438490       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-120\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-120\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-120\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-120\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-100] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d526922187894ab496f0d2f31d9a1f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3930944204330444, 'eval_wer': 1.0, 'eval_runtime': 0.7511, 'eval_samples_per_second': 13.313, 'eval_steps_per_second': 2.663, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.607734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.498682</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.438490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.393094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  eval_loss  eval_wer\n",
       "0     1.0         NaN  11.716703       1.0\n",
       "1     2.0         NaN  11.425539       1.0\n",
       "2     3.0         NaN  10.662720       1.0\n",
       "3     4.0         NaN   9.775192       1.0\n",
       "4     5.0         NaN   8.321245       1.0\n",
       "5     6.0         NaN   6.988673       1.0\n",
       "6     7.0         NaN   4.790219       1.0\n",
       "7     8.0         NaN   2.974931       1.0\n",
       "8     9.0         NaN   1.920100       1.0\n",
       "9    10.0      6.7177   1.607734       1.0\n",
       "10   11.0      6.7177   1.498682       1.0\n",
       "11   12.0      6.7177   1.438490       1.0\n",
       "12   13.0      6.7177   1.393094       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-130\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-130\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-130\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-130\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-110] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9522a802abbf4378afc04e608f0c840d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3473025560379028, 'eval_wer': 1.0, 'eval_runtime': 0.7942, 'eval_samples_per_second': 12.592, 'eval_steps_per_second': 2.518, 'epoch': 14.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.607734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.498682</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.438490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.393094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.347303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  eval_loss  eval_wer\n",
       "0     1.0         NaN  11.716703       1.0\n",
       "1     2.0         NaN  11.425539       1.0\n",
       "2     3.0         NaN  10.662720       1.0\n",
       "3     4.0         NaN   9.775192       1.0\n",
       "4     5.0         NaN   8.321245       1.0\n",
       "5     6.0         NaN   6.988673       1.0\n",
       "6     7.0         NaN   4.790219       1.0\n",
       "7     8.0         NaN   2.974931       1.0\n",
       "8     9.0         NaN   1.920100       1.0\n",
       "9    10.0      6.7177   1.607734       1.0\n",
       "10   11.0      6.7177   1.498682       1.0\n",
       "11   12.0      6.7177   1.438490       1.0\n",
       "12   13.0      6.7177   1.393094       1.0\n",
       "13   14.0      6.7177   1.347303       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-140\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-140\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-140\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-140\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-120] due to args.save_total_limit\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n",
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b177210fb5424e900aa98405950ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2923663854599, 'eval_wer': 1.0, 'eval_runtime': 0.8082, 'eval_samples_per_second': 12.373, 'eval_steps_per_second': 2.475, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.607734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.498682</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.438490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.393094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.347303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.292366</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  eval_loss  eval_wer\n",
       "0     1.0         NaN  11.716703       1.0\n",
       "1     2.0         NaN  11.425539       1.0\n",
       "2     3.0         NaN  10.662720       1.0\n",
       "3     4.0         NaN   9.775192       1.0\n",
       "4     5.0         NaN   8.321245       1.0\n",
       "5     6.0         NaN   6.988673       1.0\n",
       "6     7.0         NaN   4.790219       1.0\n",
       "7     8.0         NaN   2.974931       1.0\n",
       "8     9.0         NaN   1.920100       1.0\n",
       "9    10.0      6.7177   1.607734       1.0\n",
       "10   11.0      6.7177   1.498682       1.0\n",
       "11   12.0      6.7177   1.438490       1.0\n",
       "12   13.0      6.7177   1.393094       1.0\n",
       "13   14.0      6.7177   1.347303       1.0\n",
       "14   15.0      6.7177   1.292366       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-150\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-150\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-150\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-150\\preprocessor_config.json\n",
      "Deleting older checkpoint [e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-130] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\checkpoint-10 (score: 1.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 215.7622, 'train_samples_per_second': 5.492, 'train_steps_per_second': 0.695, 'train_loss': 5.103202921549479, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=5.103202921549479, metrics={'train_runtime': 215.7622, 'train_samples_per_second': 5.492, 'train_steps_per_second': 0.695, 'train_loss': 5.103202921549479, 'epoch': 15.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\n",
      "Configuration saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\\config.json\n",
      "Model weights saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\\pytorch_model.bin\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\\preprocessor_config.json\n",
      "Feature extractor saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\\preprocessor_config.json\n",
      "tokenizer config file saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\\tokenizer_config.json\n",
      "Special tokens file saved in e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to e:\\Singaporean_English_ASR\\output\\models\\wav2vec2\\best_model\n"
     ]
    }
   ],
   "source": [
    "# After training, save the best model to the specified directory\n",
    "trainer.save_model(best_model_dir)\n",
    "\n",
    "# Also save the processor to the best model directory\n",
    "processor.save_pretrained(best_model_dir)\n",
    "\n",
    "print(f\"Best model saved to {best_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Training Metrics:\n",
      " epoch  train_loss  eval_loss  eval_wer\n",
      "   1.0         NaN  11.716703       1.0\n",
      "   2.0         NaN  11.425539       1.0\n",
      "   3.0         NaN  10.662720       1.0\n",
      "   4.0         NaN   9.775192       1.0\n",
      "   5.0         NaN   8.321245       1.0\n",
      "   6.0         NaN   6.988673       1.0\n",
      "   7.0         NaN   4.790219       1.0\n",
      "   8.0         NaN   2.974931       1.0\n",
      "   9.0         NaN   1.920100       1.0\n",
      "  10.0      6.7177   1.607734       1.0\n",
      "  11.0      6.7177   1.498682       1.0\n",
      "  12.0      6.7177   1.438490       1.0\n",
      "  13.0      6.7177   1.393094       1.0\n",
      "  14.0      6.7177   1.347303       1.0\n",
      "  15.0      6.7177   1.292366       1.0\n"
     ]
    }
   ],
   "source": [
    "# After training, get the full metrics table\n",
    "metrics_df = pd.DataFrame(metrics_callback.metrics)\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# You can also save the metrics to a CSV file\n",
    "metrics_df.to_csv(os.path.join(output_dir, \"final_training_metrics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: input_length. If input_length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Singaporean_English_ASR\\.venv\\lib\\site-packages\\transformers\\trainer.py:1949: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx_manager = autocast(dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3c788570cf41f59d5d26fc9c020e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.716703</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.425539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.662720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.775192</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.321245</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.988673</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790219</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.974931</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.920100</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.607734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.498682</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.438490</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.393094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.347303</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>1.292366</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.7177</td>\n",
       "      <td>11.368688</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  eval_loss  eval_wer\n",
       "0     1.0         NaN  11.716703       1.0\n",
       "1     2.0         NaN  11.425539       1.0\n",
       "2     3.0         NaN  10.662720       1.0\n",
       "3     4.0         NaN   9.775192       1.0\n",
       "4     5.0         NaN   8.321245       1.0\n",
       "5     6.0         NaN   6.988673       1.0\n",
       "6     7.0         NaN   4.790219       1.0\n",
       "7     8.0         NaN   2.974931       1.0\n",
       "8     9.0         NaN   1.920100       1.0\n",
       "9    10.0      6.7177   1.607734       1.0\n",
       "10   11.0      6.7177   1.498682       1.0\n",
       "11   12.0      6.7177   1.438490       1.0\n",
       "12   13.0      6.7177   1.393094       1.0\n",
       "13   14.0      6.7177   1.347303       1.0\n",
       "14   15.0      6.7177   1.292366       1.0\n",
       "15   15.0      6.7177  11.368688       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER: 1.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "eval_results = trainer.evaluate(dataset[\"test\"])\n",
    "print(f\"Test WER: {eval_results['eval_wer']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# Define output directories\n",
    "output_dir = os.path.abspath(os.path.join(\"..\", \"..\", \"output\", \"models\", \"wav2vec2\"))\n",
    "study_name = \"wav2vec2_hyperparam_search\"\n",
    "study_storage = os.path.join(output_dir, f\"{study_name}.db\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load metric for evaluation\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "# Data collator for CTC with padding\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad inputs\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Pad labels\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# Metric computation function\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Replace -100 with pad token id\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# Model initialization function\n",
    "def create_model(trial):\n",
    "    # Get hyperparameters from trial\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-base\",\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    # Freeze feature encoder based on trial suggestion\n",
    "    if trial.suggest_categorical(\"freeze_feature_encoder\", [True, False]):\n",
    "        model.freeze_feature_encoder()\n",
    "    \n",
    "    # Freeze feature projection based on trial suggestion\n",
    "    if trial.suggest_categorical(\"freeze_feature_projection\", [True, False]):\n",
    "        if hasattr(model, \"wav2vec2\") and hasattr(model.wav2vec2, \"feature_projection\"):\n",
    "            for param in model.wav2vec2.feature_projection.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    # Freeze a portion of transformer layers\n",
    "    if hasattr(model, \"wav2vec2\") and hasattr(model.wav2vec2, \"encoder\"):\n",
    "        num_layers = len(model.wav2vec2.encoder.layers)\n",
    "        freeze_layers = trial.suggest_int(\"freeze_layers\", 0, num_layers)\n",
    "        if freeze_layers > 0:\n",
    "            for layer in model.wav2vec2.encoder.layers[:freeze_layers]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    warmup_ratio = trial.suggest_float(\"warmup_ratio\", 0.0, 0.2)\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0.0, 0.2)\n",
    "    hidden_dropout = trial.suggest_float(\"hidden_dropout\", 0.0, 0.2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    \n",
    "    # Calculate warmup steps based on ratio and total steps\n",
    "    num_train_epochs = 10  # Reduced for hyperparameter search\n",
    "    num_warmup_steps = int(len(dataset[\"train\"]) // batch_size * num_train_epochs * warmup_ratio)\n",
    "    \n",
    "    # Set model configuration\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Set training arguments\n",
    "    run_name = f\"trial_{trial.number}\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(output_dir, run_name),\n",
    "        group_by_length=True,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=num_warmup_steps,\n",
    "        save_total_limit=1,\n",
    "        push_to_hub=False,\n",
    "        logging_dir=os.path.join(output_dir, run_name, \"logs\"),\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # Initialize data collator\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(early_stopping_patience=3)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Get validation WER\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Record the best results\n",
    "    trial.set_user_attr(\"best_wer\", eval_results[\"eval_wer\"])\n",
    "    \n",
    "    # Save trial results to CSV for easier analysis\n",
    "    results_df = pd.DataFrame({\n",
    "        \"trial\": [trial.number],\n",
    "        \"learning_rate\": [learning_rate],\n",
    "        \"weight_decay\": [weight_decay],\n",
    "        \"warmup_ratio\": [warmup_ratio],\n",
    "        \"attention_dropout\": [attention_dropout],\n",
    "        \"hidden_dropout\": [hidden_dropout],\n",
    "        \"batch_size\": [batch_size],\n",
    "        \"freeze_feature_encoder\": [trial.params[\"freeze_feature_encoder\"]],\n",
    "        \"freeze_feature_projection\": [trial.params[\"freeze_feature_projection\"]],\n",
    "        \"freeze_layers\": [trial.params[\"freeze_layers\"]],\n",
    "        \"wer\": [eval_results[\"eval_wer\"]]\n",
    "    })\n",
    "    \n",
    "    # Append or create CSV file with results\n",
    "    results_path = os.path.join(output_dir, \"hyperparameter_search_results.csv\")\n",
    "    if os.path.exists(results_path):\n",
    "        results_df.to_csv(results_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "    \n",
    "    # Return validation WER (lower is better)\n",
    "    return eval_results[\"eval_wer\"]\n",
    "\n",
    "# Create an Optuna study\n",
    "def run_hyperparameter_search(n_trials=20):\n",
    "    # Create a study with the TPE sampler and median pruner\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=f\"sqlite:///{study_storage}\",\n",
    "        load_if_exists=True,\n",
    "        direction=\"minimize\",  # Minimize WER\n",
    "        sampler=TPESampler(seed=42),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Print study statistics\n",
    "    print(f\"\\nNumber of finished trials: {len(study.trials)}\")\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  Value (WER): {best_trial.value:.4f}\")\n",
    "    print(\"  Params:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Get best parameters and train final model\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Create a results dataframe with all trials\n",
    "    trials_df = pd.DataFrame([\n",
    "        {\n",
    "            \"trial\": t.number,\n",
    "            **t.params,\n",
    "            \"wer\": t.value,\n",
    "            \"state\": t.state,\n",
    "        }\n",
    "        for t in study.trials if t.state.is_finished()\n",
    "    ])\n",
    "    \n",
    "    # Sort by WER (ascending)\n",
    "    trials_df = trials_df.sort_values(\"wer\")\n",
    "    \n",
    "    # Save complete results\n",
    "    trials_df.to_csv(os.path.join(output_dir, \"all_trials_results.csv\"), index=False)\n",
    "    \n",
    "    # Display top 5 results\n",
    "    print(\"\\nTop 5 configurations:\")\n",
    "    display(trials_df.head(5))\n",
    "    \n",
    "    # Create visualizations\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        from optuna.visualization import plot_param_importances, plot_optimization_history\n",
    "        \n",
    "        # Parameter importance plot\n",
    "        importance_fig = plot_param_importances(study)\n",
    "        importance_fig.write_image(os.path.join(output_dir, \"param_importance.png\"))\n",
    "        \n",
    "        # Optimization history plot\n",
    "        history_fig = plot_optimization_history(study)\n",
    "        history_fig.write_image(os.path.join(output_dir, \"optimization_history.png\"))\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Skipping visualizations - matplotlib or plotly not available\")\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "def train_final_model(best_params):\n",
    "    print(\"\\nTraining final model with best hyperparameters...\")\n",
    "    \n",
    "    # Create model with best configuration\n",
    "    class ModelCreator:\n",
    "        def suggest_categorical(self, name, choices):\n",
    "            return best_params[name]\n",
    "        \n",
    "        def suggest_int(self, name, low, high):\n",
    "            return best_params[name]\n",
    "    \n",
    "    trial = ModelCreator()\n",
    "    model = create_model(trial)\n",
    "    \n",
    "    # Calculate warmup steps based on best parameters\n",
    "    num_train_epochs = 30  # Full training\n",
    "    batch_size = best_params[\"batch_size\"]\n",
    "    warmup_ratio = best_params[\"warmup_ratio\"]\n",
    "    num_warmup_steps = int(len(dataset[\"train\"]) // batch_size * num_train_epochs * warmup_ratio)\n",
    "    \n",
    "    # Set training arguments for final model\n",
    "    final_output_dir = os.path.join(output_dir, \"final_model\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=final_output_dir,\n",
    "        group_by_length=True,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        fp16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        warmup_steps=num_warmup_steps,\n",
    "        save_total_limit=2,\n",
    "        push_to_hub=False,\n",
    "        logging_dir=os.path.join(final_output_dir, \"logs\"),\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "    \n",
    "    # Initialize data collator\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "    \n",
    "    # Setup trainer for final model\n",
    "    final_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    final_trainer.train()\n",
    "    \n",
    "    # Evaluate on test set if available\n",
    "    if \"test\" in dataset:\n",
    "        test_results = final_trainer.evaluate(dataset[\"test\"])\n",
    "        print(f\"Test WER: {test_results['eval_wer']:.4f}\")\n",
    "    \n",
    "    # Save final model and processor\n",
    "    final_trainer.save_model(final_output_dir)\n",
    "    processor.save_pretrained(final_output_dir)\n",
    "    \n",
    "    print(f\"Final model saved to {final_output_dir}\")\n",
    "    \n",
    "    return final_trainer\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load processor (ensure this matches your dataset preprocessing)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    \n",
    "    # Assuming dataset is already loaded and preprocessed\n",
    "    # Run hyperparameter search\n",
    "    best_params = run_hyperparameter_search(n_trials=20)\n",
    "    \n",
    "    # Train final model with best hyperparameters\n",
    "    final_trainer = train_final_model(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
