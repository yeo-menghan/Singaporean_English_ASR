{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: soundfile in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: pandas in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: tqdm in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: msgpack>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: joblib>=0.14 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: cffi>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pycparser in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install librosa soundfile pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data...\n",
      "Dropped columns: ['__key__', '__url__']\n",
      "Extracting 'original_text' from json column...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9988.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing json in row 0: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 1, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'Mary and her family were moving to another city.', 'read_text': 'Mary and her family were moving to another city'}\n",
      "Error processing json in row 1: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 57, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'The witch put a spell on the prince, but it was the wrong one.', 'read_text': 'the witch put a spell on the prince but it was the wrong one'}\n",
      "Error processing json in row 2: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 108, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'According to the Japanese doctors, it is impossible to determine how long my dad will remain comatose.', 'read_text': 'according to the Japanese doctors it is impossible to determine how long my dad will remain comatose'}\n",
      "Error processing json in row 3: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 158, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'One in five of the incidents happened during pool parties.', 'read_text': 'one in five of the incidents happened during pool parties'}\n",
      "Error processing json in row 4: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 209, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'Regardless, as free parking on Sunday and in general becomes increasingly endangered, so might our delicate social fabric.', 'read_text': 'regardless as free parking on Sunday and in general becomes increasingly endangered so might our delicate social fabric'}\n",
      "Error processing json in row 5: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 259, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'Get it from someone trustworthy, with good reviews.', 'read_text': 'get it from someone trustworthy with good reviews'}\n",
      "Error processing json in row 6: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 310, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'Even when we say active ageing, people tend to associate it with the physical aspect.', 'read_text': 'even when we say active ageing people tend to associate it with the physical ** aspect'}\n",
      "Error processing json in row 7: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 362, 'SessionID': 0, 'SpeakerID': 402, 'original_text': 'Having a regular primary care team will ensure better continuity of care for patients.', 'read_text': 'having a regular primary care team will ensure better continuity of care for patients'}\n",
      "Error processing json in row 8: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 12, 'SessionID': 0, 'SpeakerID': 566, 'original_text': 'Just as they were trying to find a tool or two that they could use to fend off the dogs, one of the dogs spotted them.', 'read_text': 'just as they were trying to find A tool or two that they could use to fend off the dogs one of the dogs spotted them'}\n",
      "Error processing json in row 9: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw json content: {'ChannelID': 0, 'RecordingID': 63, 'SessionID': 0, 'SpeakerID': 566, 'original_text': 'Rubber boots are not popular in Singapore despite our rainy weather.', 'read_text': 'rubber boots are not popular in Singapore despite our rainy weather'}\n",
      "Dropped 'json' column after extracting text\n",
      "Converting all text to lowercase...\n",
      "Standardizing audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed train data to ../processed\\train_preprocessed.csv and ../processed\\train_preprocessed.json\n",
      "Number of rows with extracted text: 0 out of 10\n",
      "Skipping validation, file not found: ../raw\\validation_data.csv\n",
      "Skipping test, file not found: ../raw\\test_data.csv\n",
      "Preprocessing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input directory where the saved data is located\n",
    "input_dir = \"../raw\"\n",
    "# Output directory for preprocessed data\n",
    "output_dir = \"../processed\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Target sampling rate for audio standardization\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "# Target audio duration in seconds\n",
    "TARGET_DURATION = 5  # 5 seconds\n",
    "# Normalization factor\n",
    "NORMALIZATION_FACTOR = 0.95\n",
    "\n",
    "# Process each data file\n",
    "for file_type in [\"train\", \"validation\", \"test\"]:\n",
    "    # Try to find the file\n",
    "    input_file = os.path.join(input_dir, f\"{file_type}_data.csv\")\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Skipping {file_type}, file not found: {input_file}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing {file_type} data...\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Step 1: Drop key and url columns if they exist\n",
    "    columns_to_drop = [col for col in df.columns if col in ['key', 'url'] or 'key' in col.lower() or 'url' in col.lower()]\n",
    "    if columns_to_drop:\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        print(f\"Dropped columns: {columns_to_drop}\")\n",
    "    \n",
    "    # Step 2: Extract 'original_text' from the json column\n",
    "    print(\"Extracting 'original_text' from json column...\")\n",
    "    \n",
    "    # Create a new text column\n",
    "    df['text'] = None\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if 'json' in df.columns:\n",
    "            try:\n",
    "                json_str = row['json']\n",
    "                \n",
    "                # Handle different formats of the json data\n",
    "                if isinstance(json_str, str):\n",
    "                    if json_str.strip().startswith('{'):\n",
    "                        # It's a JSON string\n",
    "                        json_data = json.loads(json_str)\n",
    "                    else:\n",
    "                        # It might be a string representation of a Python dict\n",
    "                        json_data = eval(json_str)\n",
    "                elif isinstance(json_str, dict):\n",
    "                    # It's already a dictionary\n",
    "                    json_data = json_str\n",
    "                else:\n",
    "                    print(f\"Unknown JSON format in row {idx}: {type(json_str)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract original_text\n",
    "                if 'original_text' in json_data:\n",
    "                    df.at[idx, 'text'] = json_data['original_text']\n",
    "                    print(f\"Found text: {json_data['original_text']}\")\n",
    "                else:\n",
    "                    print(f\"'original_text' not found in json data: {json_data.keys()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing json in row {idx}: {e}\")\n",
    "                print(f\"Raw json content: {row['json']}\")\n",
    "    \n",
    "    # Now drop the json column\n",
    "    if 'json' in df.columns:\n",
    "        df = df.drop(columns=['json'])\n",
    "        print(\"Dropped 'json' column after extracting text\")\n",
    "    \n",
    "    # Step 3: Ensure all text is lowercase\n",
    "    print(\"Converting all text to lowercase...\")\n",
    "    if 'text' in df.columns:\n",
    "        df['text'] = df['text'].str.lower()\n",
    "    \n",
    "    # Step 4: Standardize audio files\n",
    "    print(\"Standardizing audio files...\")\n",
    "    \n",
    "    # Create audio directory\n",
    "    audio_dir = os.path.join(output_dir, f\"{file_type}_audio\")\n",
    "    os.makedirs(audio_dir, exist_ok=True)\n",
    "    \n",
    "    # New column for standardized audio paths\n",
    "    df['standardized_audio'] = None\n",
    "    \n",
    "    # Process each audio file\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if 'flac' in df.columns and pd.notna(row['flac']):\n",
    "            try:\n",
    "                # Load the audio file\n",
    "                audio_path = row['flac']\n",
    "                \n",
    "                # Check if it's a string (path) or something else\n",
    "                if isinstance(audio_path, str) and os.path.exists(audio_path):\n",
    "                    # Load the audio file\n",
    "                    audio, sr = librosa.load(audio_path, sr=None)\n",
    "                    \n",
    "                    # Resample if needed\n",
    "                    if sr != TARGET_SAMPLING_RATE:\n",
    "                        audio = librosa.resample(audio, orig_sr=sr, target_sr=TARGET_SAMPLING_RATE)\n",
    "                    \n",
    "                    # Normalize audio\n",
    "                    if np.abs(audio).max() > 0:\n",
    "                        audio = audio / np.abs(audio).max() * NORMALIZATION_FACTOR\n",
    "                    \n",
    "                    # Step 5: Handle variable length - pad or trim\n",
    "                    target_length = int(TARGET_DURATION * TARGET_SAMPLING_RATE)\n",
    "                    \n",
    "                    if len(audio) < target_length:\n",
    "                        # Pad with zeros if too short\n",
    "                        padding = np.zeros(target_length - len(audio))\n",
    "                        audio = np.concatenate([audio, padding])\n",
    "                    elif len(audio) > target_length:\n",
    "                        # Trim if too long (take the first TARGET_DURATION seconds)\n",
    "                        audio = audio[:target_length]\n",
    "                    \n",
    "                    # Save the standardized audio\n",
    "                    output_audio_path = os.path.join(audio_dir, f\"{file_type}_{idx}.flac\")\n",
    "                    sf.write(output_audio_path, audio, TARGET_SAMPLING_RATE)\n",
    "                    \n",
    "                    # Update the dataframe\n",
    "                    df.at[idx, 'standardized_audio'] = output_audio_path\n",
    "                else:\n",
    "                    print(f\"Audio file not found or invalid: {audio_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing audio in row {idx}: {e}\")\n",
    "    \n",
    "    # Drop the original flac column\n",
    "    if 'flac' in df.columns:\n",
    "        df = df.drop(columns=['flac'])\n",
    "    \n",
    "    # Save the preprocessed data\n",
    "    output_file = os.path.join(output_dir, f\"{file_type}_preprocessed.csv\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Also save as JSON for convenience\n",
    "    output_json = os.path.join(output_dir, f\"{file_type}_preprocessed.json\")\n",
    "    df.to_json(output_json, orient='records', indent=2)\n",
    "    \n",
    "    print(f\"Saved preprocessed {file_type} data to {output_file} and {output_json}\")\n",
    "    \n",
    "    # Verify text extraction\n",
    "    non_null_text = df['text'].count()\n",
    "    print(f\"Number of rows with extracted text: {non_null_text} out of {len(df)}\")\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
