{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: packaging in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: xxhash in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: aiohttp in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: filelock in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: requests in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: colorama in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from soundfile) (2.1.3)\n",
      "Requirement already satisfied: pycparser in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: msgpack>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: joblib>=0.14 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: pooch>=1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (2.1.3)\n",
      "Requirement already satisfied: soxr>=0.3.2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: packaging in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in e:\\singaporean_english_asr\\.venv\\lib\\site-packages (from openpyxl) (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets\n",
    "! pip install huggingface_hub\n",
    "! pip install pandas\n",
    "! pip install soundfile\n",
    "! pip install librosa\n",
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac132faaf2f2407cb6e26d5ea94f559e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA device index\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Full Dataset (Not Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fc154852a64676b086f630af57f8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "import os\n",
    "\n",
    "dataset_name = \"recursal/reprocessed_singapore_national_speech_corpus\"\n",
    "custom_cache_dir = \"../raw\"  \n",
    "dataset = load_dataset(dataset_name, cache_dir=custom_cache_dir)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream & Load samples by intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0143d3e97a9243318663dae8fb59f7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added example 0 (sample 1/10)\n",
      "Added example 50 (sample 2/10)\n",
      "Added example 100 (sample 3/10)\n",
      "Added example 150 (sample 4/10)\n",
      "Added example 200 (sample 5/10)\n",
      "Added example 250 (sample 6/10)\n",
      "Added example 300 (sample 7/10)\n",
      "Added example 350 (sample 8/10)\n",
      "Added example 400 (sample 9/10)\n",
      "Added example 450 (sample 10/10)\n",
      "Saved train data with 10 examples (sampled every 50 examples)\n",
      "Output files: ../raw\\train_data.xlsx, ../raw\\train_data.csv, and ../raw\\train_data.json\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "dataset_name = \"recursal/reprocessed_singapore_national_speech_corpus\"\n",
    "local_save_dir = \"../raw\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(local_save_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset = load_dataset(dataset_name, streaming=True)\n",
    "\n",
    "# Number of examples to collect per split\n",
    "num_examples = 10\n",
    "\n",
    "# Sampling interval - take every Nth example\n",
    "interval = 50\n",
    "\n",
    "# Process each split\n",
    "for split_name, split_dataset in dataset.items():\n",
    "    # Create a subdirectory for this split\n",
    "    split_dir = os.path.join(local_save_dir, split_name)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a list to store all example data\n",
    "    examples_data = []\n",
    "    \n",
    "    # Take samples at regular intervals\n",
    "    for i, example in enumerate(split_dataset):\n",
    "        # Only sample at the specified interval\n",
    "        if i % interval != 0:\n",
    "            continue\n",
    "            \n",
    "        # Process the audio file\n",
    "        flac_data = example['flac']\n",
    "        \n",
    "        # Check the format of the flac data\n",
    "        if isinstance(flac_data, dict) and 'array' in flac_data and 'path' in flac_data:\n",
    "            # Extract the array and sampling rate\n",
    "            audio_array = flac_data['array']\n",
    "            sampling_rate = flac_data.get('sampling_rate', 16000)\n",
    "            \n",
    "            # Create a filename based on the original or use a sequential number\n",
    "            filename = os.path.basename(flac_data['path']) if 'path' in flac_data else f\"{split_name}_{len(examples_data)}.flac\"\n",
    "            audio_path = os.path.join(split_dir, filename)\n",
    "            \n",
    "            # Save the audio array to a file\n",
    "            sf.write(audio_path, audio_array, sampling_rate)\n",
    "        else:\n",
    "            # If it's already a path, copy the file\n",
    "            audio_path = flac_data\n",
    "            filename = os.path.basename(audio_path)\n",
    "            destination_path = os.path.join(split_dir, filename)\n",
    "            shutil.copy(audio_path, destination_path)\n",
    "            audio_path = destination_path\n",
    "        \n",
    "        # Create a dictionary with all the example data\n",
    "        example_dict = dict(example)\n",
    "        \n",
    "        # Replace the flac data with the path to the saved file\n",
    "        example_dict['flac'] = audio_path\n",
    "        \n",
    "        # Log which example we're processing\n",
    "        print(f\"Added example {i} (sample {len(examples_data)+1}/{num_examples})\")\n",
    "        \n",
    "        # Add to our list of examples\n",
    "        examples_data.append(example_dict)\n",
    "        \n",
    "        # Stop after collecting enough examples\n",
    "        if len(examples_data) >= num_examples:\n",
    "            break\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(examples_data)\n",
    "    \n",
    "    # Save as Excel (excluding large binary data if needed)\n",
    "    excel_path = os.path.join(local_save_dir, f\"{split_name}_data.xlsx\")\n",
    "    export_df = df.copy()\n",
    "    for col in export_df.columns:\n",
    "        # Convert any complex columns to string representation\n",
    "        if export_df[col].dtype == 'object':\n",
    "            export_df[col] = export_df[col].apply(lambda x: str(x) if not isinstance(x, (str, int, float, bool, type(None))) else x)\n",
    "    export_df.to_excel(excel_path, index=False)\n",
    "    \n",
    "    # Save as CSV\n",
    "    csv_path = os.path.join(local_save_dir, f\"{split_name}_data.csv\")\n",
    "    export_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save as JSON\n",
    "    json_path = os.path.join(local_save_dir, f\"{split_name}_data.json\")\n",
    "    \n",
    "    # Handle complex objects for JSON serialization\n",
    "    json_data = []\n",
    "    for example in examples_data:\n",
    "        json_example = {}\n",
    "        for k, v in example.items():\n",
    "            # Skip numpy arrays and other complex objects\n",
    "            if isinstance(v, (str, int, float, bool, type(None))):\n",
    "                json_example[k] = v\n",
    "            elif isinstance(v, dict):\n",
    "                # Convert sub-dictionaries properly\n",
    "                json_example[k] = {str(sk): str(sv) if not isinstance(sv, (str, int, float, bool, type(None))) else sv \n",
    "                                 for sk, sv in v.items()}\n",
    "            else:\n",
    "                json_example[k] = str(v)\n",
    "        json_data.append(json_example)\n",
    "    \n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved {split_name} data with {len(examples_data)} examples (sampled every {interval} examples)\")\n",
    "    print(f\"Output files: {excel_path}, {csv_path}, and {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train set with 8 examples\n",
      "Output files: ../processed_splits\\train_data.xlsx, ../processed_splits\\train_data.csv, and ../processed_splits\\train_data.json\n",
      "Created validation set with 1 examples\n",
      "Output files: ../processed_splits\\validation_data.xlsx, ../processed_splits\\validation_data.csv, and ../processed_splits\\validation_data.json\n",
      "Created test set with 1 examples\n",
      "Output files: ../processed_splits\\test_data.xlsx, ../processed_splits\\test_data.csv, and ../processed_splits\\test_data.json\n",
      "\n",
      "Split complete:\n",
      "Original train set: 10 examples\n",
      "New training set: 8 examples (80.0%)\n",
      "Validation set: 1 examples (10.0%)\n",
      "Test set: 1 examples (10.0%)\n",
      "\n",
      "All splits are saved in: ../processed_splits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "raw_dir = \"../raw\"\n",
    "processed_dir = \"../processed_splits\"  # New parent directory at same level as raw\n",
    "original_train_dir = os.path.join(raw_dir, \"train\")\n",
    "original_train_excel = os.path.join(raw_dir, \"train_data.xlsx\")\n",
    "original_train_json = os.path.join(raw_dir, \"train_data.json\")\n",
    "\n",
    "# Create new processed directory\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Create new directories for the split inside processed_dir\n",
    "split_train_dir = os.path.join(processed_dir, \"train\")\n",
    "val_dir = os.path.join(processed_dir, \"validation\")\n",
    "test_dir = os.path.join(processed_dir, \"test\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [split_train_dir, val_dir, test_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Load original training data\n",
    "train_df = pd.read_excel(original_train_excel)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(original_train_json, 'r') as f:\n",
    "    train_json = json.load(f)\n",
    "\n",
    "# Make sure the DataFrame and JSON have the same number of records\n",
    "assert len(train_df) == len(train_json), \"Excel and JSON files have different numbers of records\"\n",
    "\n",
    "# Split the data: 80% train, 10% validation, 10% test\n",
    "train_indices, temp_indices = train_test_split(range(len(train_df)), test_size=0.2, random_state=42)\n",
    "val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "# Function to create the split datasets\n",
    "def create_split(indices, df, json_data, target_dir, split_name):\n",
    "    # Create subdirectory\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Filter data\n",
    "    split_df = df.iloc[indices].reset_index(drop=True)\n",
    "    split_json = [json_data[i] for i in indices]\n",
    "    \n",
    "    # Copy audio files to the new directory\n",
    "    for i, row in split_df.iterrows():\n",
    "        audio_path = row['flac']\n",
    "        if isinstance(audio_path, str) and os.path.exists(audio_path):\n",
    "            filename = os.path.basename(audio_path)\n",
    "            new_path = os.path.join(target_dir, filename)\n",
    "            shutil.copy(audio_path, new_path)\n",
    "            \n",
    "            # Update path in the DataFrame and JSON to reflect the new location\n",
    "            split_df.at[i, 'flac'] = new_path\n",
    "            split_json[i]['flac'] = new_path\n",
    "    \n",
    "    # Save as Excel in the processed directory\n",
    "    excel_path = os.path.join(processed_dir, f\"{split_name}_data.xlsx\")\n",
    "    split_df.to_excel(excel_path, index=False)\n",
    "    \n",
    "    # Save as CSV in the processed directory\n",
    "    csv_path = os.path.join(processed_dir, f\"{split_name}_data.csv\")\n",
    "    split_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save as JSON in the processed directory\n",
    "    json_path = os.path.join(processed_dir, f\"{split_name}_data.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(split_json, f, indent=2)\n",
    "    \n",
    "    print(f\"Created {split_name} set with {len(indices)} examples\")\n",
    "    print(f\"Output files: {excel_path}, {csv_path}, and {json_path}\")\n",
    "    \n",
    "    return split_df, split_json\n",
    "\n",
    "# Create the splits\n",
    "train_split_df, train_split_json = create_split(train_indices, train_df, train_json, split_train_dir, \"train\")\n",
    "val_split_df, val_split_json = create_split(val_indices, train_df, train_json, val_dir, \"validation\")\n",
    "test_split_df, test_split_json = create_split(test_indices, train_df, train_json, test_dir, \"test\")\n",
    "\n",
    "print(f\"\\nSplit complete:\")\n",
    "print(f\"Original train set: {len(train_df)} examples\")\n",
    "print(f\"New training set: {len(train_split_df)} examples ({len(train_split_df)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_split_df)} examples ({len(val_split_df)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_split_df)} examples ({len(test_split_df)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"\\nAll splits are saved in: {processed_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
