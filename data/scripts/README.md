# Data Processing and Preprocessing Pipelines
This repository contains two Jupyter notebook scripts designed for data preparation and preprocessing workflows for audio and text datasets. The notebooks are crucial steps in preparing data for machine learning models, particularly for Automatic Speech Recognition (ASR) tasks. Below is a detailed overview of the two notebooks:

## 1. download_dataset.ipynb
### Overview
This notebook is responsible for downloading and sampling an audio dataset from an online source. It processes the dataset by sampling intervals, downloading audio files, and saving the data in multiple formats (JSON, CSV, and Excel). Additionally, it verifies data integrity and organizes the data for further preprocessing.

### Key Features
1. Dependencies Installation:
- Installs necessary Python libraries such as datasets, huggingface_hub, pandas, librosa, seaborn, etc.
- Ensures compatibility with the environment.
2. Dataset Streaming and Sampling:
- Downloads the dataset in streaming mode using the Hugging Face datasets library.
- Samples data at regular intervals (e.g., every 50th record) to reduce the dataset size for testing or training purposes.
3. Audio File Handling:
- Processes audio files (.flac format) to ensure compatibility with downstream tasks.
- Saves audio files locally in a structured directory.
4. Data Export:
- Outputs sampled data into multiple formats (JSON, CSV, and Excel) for analysis or further processing.
5. Data Exploration:
- Provides exploratory insights (e.g., sample text and audio file paths) from the dataset using Pandas and visualizations.

### Outputs
- Processed audio files saved in a directory structure.
- Metadata files (train_data.json, train_data.csv, train_data.xlsx, etc.) for each split (train, validation, test).
- Logs for processed and saved data.


## data_preprocessing.ipynb
### Overview
This notebook preprocesses the raw audio and metadata files generated by the download_dataset.ipynb. It focuses on standardizing audio files, cleaning metadata, and preparing the dataset for machine learning models.

### Key Features
1. Dependencies Installation:
- Installs core libraries like librosa, soundfile, pandas, numpy, and tqdm.
2. JSON Metadata Preprocessing:
- Loads JSON files containing metadata.
- Extracts and cleans relevant fields (e.g., original_text) and standardizes the text (e.g., converting to lowercase).
- Updates file paths for processed audio.
3. Audio Standardization:
- Resamples audio files to a target sample rate (e.g., 16 kHz).
- Normalizes audio signals to ensure consistent amplitude.
- Pads audio files shorter than a minimum duration (e.g., 1 second) to ensure uniformity.
4. Data Organization:
- Saves processed audio files in a structured directory for each split (train, validation, test).
- Updates JSON metadata and exports it alongside CSV files.
5. Statistics:
- Computes and logs statistics for processed audio files, including: Minimum, maximum, and mean durations of audio files.

### Outputs
- Preprocessed audio files saved in the ../processed directory.
- Updated metadata files (train_data.json, validation_data.json, etc.) with cleaned and updated paths.
- CSV metadata files for easy inspection or training pipeline integration.


## Running the Notebooks
### Download Dataset:
- Run download_dataset.ipynb to stream and sample the dataset.
- Ensure you configure the dataset name and save paths correctly.

### Preprocess Dataset:
- Run data_preprocessing.ipynb to clean and standardize the data.
- Update directory paths (raw_splits_dir, processed_dir) if needed.
